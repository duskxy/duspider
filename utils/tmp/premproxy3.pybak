import aiohttp
import asyncio
from lxml import etree
import jsbeautifier
import re
import requests
import sys
import queue
import threading
import os
import aiofiles

if os.path.exists("proxyver.txt"):
    os.remove("proxyver.txt")



headers = {"Host": "premproxy.com",
               "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:56.0) Gecko/20100101 Firefox/56.0",
               "Referer": "https://premproxy.com"
               }
   
lock = asyncio.Lock()

q = asyncio.Queue()

#同步
def verpro(q):
    vc = q.get(timeout=0.2)
    proxies = {"http":vc}
    try:
        tv = requests.get("http://www.freebuf.com",proxies=proxies,timeout=6)
        if tv.status_code == 200:
            with lock:
                with open("proxyver.txt","a+") as ww:
                    ww.write(vc + '\n')
                    ww.flush()
    except Exception as e:
        pass

#异步
async def getver(q):
    qq = await q.get()
    print("q为{}".format(qq))
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get("http://www.proxy.com",timeout=5) as resp:
                stu = resp.status
                print(stu)
                if stu == 200:
                    print("可用代理{}".format(qq))
                    async with aiofiles.open("proxyver.txt",mode="a+") as f:
                        await f.write(qq + "\n")
                        await f.flush()
             
    except Exception as e:
        print(e)            
            



async def get(url,call_parse=None):
    
    async with aiohttp.ClientSession(headers=headers) as session:
        async with session.get(url) as resp:
            txt = await resp.text()
            stu = resp.status
            if stu == 404:
                print("爬取完毕")
                while not q.empty():
                    print("开始验证代理")
                    tasks = [getver(q) for i in range(50)]
                    await asyncio.wait(tasks)        
                print("队列大小{}".format(q.qsize()))
                sys.exit()
            p = etree.HTML(txt)
            ipp = p.xpath('//*[@class="anon" or @class="transp"]/td[1]/text()')
            jquery = "https://premproxy.com" + re.findall(r'<script src="(.*?)"></script>',txt)[1]
            async with session.get(jquery) as rejq:
                jqcont = await rejq.text()
                beau = jsbeautifier.beautify(jqcont)
                for i in ipp:
                    ip = i.replace(":","").strip()
                    po = re.findall('value=\"'+ip+'\|(.*?)\"',txt)[0]
                    try:
                        pot =  re.findall('\.'+po+'\\\\\'\)\.html\((.*?)\)',beau)[0].strip()
                    except Exception as e:
                        pot  = "8080"
                    v = i + pot
                    print(v)
                    await q.put(v)
       

def parse_url():
    pass


if __name__ == "__main__":
    i = 1
    u = "https://premproxy.com/list/{}.htm"
    while True:
        query = str(i).zfill(2) if i <10 else i
        url = u.format(query)
        print("第{}页".format(query))
        result = asyncio.ensure_future(get(url)) 
        ev = asyncio.get_event_loop()
        ev.run_until_complete(result)
        ev.close()
        i += 1
